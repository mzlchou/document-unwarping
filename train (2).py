# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOL9rTguvmj-G8HZ5obKL7FFYYx4nxpH
"""

!nvidia-smi
import torch
print(f"\nPyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

!pip install -q timm pytorch-msssim tensorboard

from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted")

import os

GITHUB_USERNAME = "mzlchou"
GITHUB_REPO = "document-unwarping"

# Clone the repo
repo_url = f"https://github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git"
print(f"Cloning from: {repo_url}")

if os.path.exists(GITHUB_REPO):
    !rm -rf {GITHUB_REPO}

!git clone {repo_url}

# Navigate into the repo
os.chdir(GITHUB_REPO)
print(f"Repository cloned to: {os.getcwd()}")

# Show files
print("\nFiles in repo:")
!ls -la

from final.model2 import DocumentUnwarpModel
from dataset_loader import get_dataloaders, visualize_batch

print("Successfully imported model and dataset_loader from your GitHub repo!")

import os

# Path to your ZIP file in Google Drive
ZIP_PATH = '/content/drive/MyDrive/renders.zip'

# Check if ZIP exists in Drive
if os.path.exists(ZIP_PATH):
    print(f"‚úì Found renders.zip in Google Drive")

    # Extract it to current directory
    print("Extracting dataset...")
    !unzip -q {ZIP_PATH}

    # Set the data directory
    DATA_DIR = 'renders/synthetic_data_pitch_sweep'

    # Verify extraction worked
    if os.path.exists(DATA_DIR):
        print(f"‚úì Dataset extracted successfully!")
        print(f"Dataset location: {DATA_DIR}")

        # Show what's inside
        print("\nDataset contents:")
        !ls {DATA_DIR}

        # Count files in each folder
        print("\nFile counts:")
        for folder in ['rgb', 'ground_truth', 'border', 'uv', 'depth']:
            folder_path = f"{DATA_DIR}/{folder}"
            if os.path.exists(folder_path):
                count = len(os.listdir(folder_path))
                print(f"  {folder}/: {count} files")
    else:
        print("‚úó Extraction failed - check ZIP structure")
        print("Expected structure inside ZIP:")
        print("  renders/")
        print("    ‚îî‚îÄ‚îÄ synthetic_data_pitch_sweep/")
        print("        ‚îú‚îÄ‚îÄ rgb/")
        print("        ‚îú‚îÄ‚îÄ ground_truth/")
        print("        ‚îî‚îÄ‚îÄ ...")
else:
    print(f"‚úó ZIP not found at: {ZIP_PATH}")
    print("\nPlease check:")
    print("1. Is the file named exactly 'renders.zip'?")
    print("2. Is it in the root of MyDrive (not in a subfolder)?")
    print("\nCurrent Drive contents:")
    !ls /content/drive/MyDrive/

print("Testing data loading...")

train_loader, val_loader = get_dataloaders(
    data_dir=DATA_DIR,
    batch_size=2,
    img_size=(256, 256),  # Small for quick test
    use_border=True
)

print(f"‚úì Train batches: {len(train_loader)}")
print(f"‚úì Val batches: {len(val_loader)}")

# Visualize one batch
import matplotlib.pyplot as plt

batch = next(iter(train_loader))
visualize_batch(batch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Create model
model = DocumentUnwarpModel(flow_scale=2.0).to(device)
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"‚úì Model created with {num_params:,} parameters")

# Test forward pass
x = torch.randn(1, 3, 256, 256).to(device)
with torch.no_grad():
    rectified, flow, grid = model(x)

print(f"\n‚úì Model test successful!")
print(f"  Input:  {x.shape}")
print(f"  Output: {rectified.shape}")
print(f"  Flow:   {flow.shape}")

CONFIG = {
    'batch_size': 4,         # Adjust based on GPU (T4: 4-8, A100: 16-32)
    'img_size': 512,         # Can use 256 for faster training
    'num_epochs': 50,        # Train longer for better results
    'learning_rate': 5e-5,   # Lower LR for fine details
    'save_every': 5,         # Save checkpoint every N epochs
    'flow_scale': 2.0,       # Allow larger flow displacements
}

print("Training Configuration:")
for key, value in CONFIG.items():
    print(f"  {key}: {value}")

!pip install -q pytorch-msssim

print("‚úì Dependencies installed")

# Additional imports for training
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm.notebook import tqdm
from pathlib import Path
import matplotlib.pyplot as plt

# Check for SSIM
try:
    from pytorch_msssim import ssim
    SSIM_AVAILABLE = True
    print("‚úì SSIM available - using combined L1 + SSIM loss")
except:
    SSIM_AVAILABLE = False
    print("‚ö†Ô∏è  SSIM not available - using L1 only")

class CombinedLoss(nn.Module):
    """
    Enhanced loss with L1 + SSIM + Perceptual components.

    - L1 (20%): Pixel-level accuracy
    - SSIM (50%): Structural similarity (geometry)
    - Perceptual (30%): Detail preservation (text!)
    - Masking: Ignores background, focuses on document
    """
    def __init__(self, l1_weight=0.2, ssim_weight=0.5, perceptual_weight=0.3):
        super().__init__()
        self.l1_weight = l1_weight
        self.ssim_weight = ssim_weight if SSIM_AVAILABLE else 0.0
        self.perceptual_weight = perceptual_weight
        self.l1 = nn.L1Loss(reduction='none')

    def forward(self, pred, target, mask=None, perceptual_fn=None):
        total_loss = 0.0

        # L1 Loss with masking
        if self.l1_weight > 0:
            l1_loss = self.l1(pred, target)
            if mask is not None:
                # Apply mask to focus on document pixels only
                mask_expanded = mask.expand_as(l1_loss)  # [B, 1, H, W] ‚Üí [B, 3, H, W]
                l1_loss = (l1_loss * mask_expanded).sum() / (mask_expanded.sum() + 1e-8)
            else:
                l1_loss = l1_loss.mean()
            total_loss += self.l1_weight * l1_loss

        # SSIM Loss (structural similarity - better for geometry!)
        if SSIM_AVAILABLE and self.ssim_weight > 0:
            ssim_val = ssim(pred, target, data_range=1.0)
            ssim_loss = 1 - ssim_val  # Convert to loss
            total_loss += self.ssim_weight * ssim_loss

        # Perceptual Loss (detail preservation - preserves text!)
        if self.perceptual_weight > 0 and perceptual_fn is not None:
            perc_loss = perceptual_fn(pred, target)
            total_loss += self.perceptual_weight * perc_loss

        return total_loss

print("‚úì Enhanced loss function defined")
import torchvision.models as models

class PerceptualLoss(nn.Module):
    """VGG-based perceptual loss to preserve high-frequency details."""
    def __init__(self):
        super().__init__()
        # Use VGG16 features (layers that capture edges/textures)
        vgg = models.vgg16(pretrained=True).features[:16]
        self.vgg = vgg.eval()
        for param in self.vgg.parameters():
            param.requires_grad = False

    def forward(self, pred, target):
        pred_features = self.vgg(pred)
        target_features = self.vgg(target)
        return F.mse_loss(pred_features, target_features)

# Create perceptual loss
try:
    perceptual_loss = PerceptualLoss().to(device)
    print("‚úì Perceptual loss created (helps preserve text!)")
except Exception as e:
    print(f"‚ö†Ô∏è  Perceptual loss failed: {e}")
    print("   Will use L1+SSIM only")

# ============================================================
# TRAINING SETUP
# ============================================================

# Create checkpoint directory
checkpoint_dir = Path('checkpoints')
checkpoint_dir.mkdir(exist_ok=True)

print("="*60)
print("Loading Full Dataset")
print("="*60)

# Load FULL dataset for training (not the test batch)
train_loader, val_loader = get_dataloaders(
    data_dir=DATA_DIR,
    batch_size=CONFIG['batch_size'],
    img_size=(CONFIG['img_size'], CONFIG['img_size']),
    use_border=True,  # Enable masking!
    num_workers=2
)

print(f"‚úì Train batches: {len(train_loader)}")
print(f"‚úì Val batches: {len(val_loader)}")
print(f"‚úì Train samples: {len(train_loader.dataset)}")
print(f"‚úì Val samples: {len(val_loader.dataset)}")

print("\n" + "="*60)
print("Creating Model & Optimizer")
print("="*60)

# Create model (fresh one for training)
model = DocumentUnwarpModel(flow_scale=CONFIG['flow_scale']).to(device)

num_params = sum(p.numel() for p in model.parameters())
print(f"‚úì Model parameters: {num_params:,}")

# Loss and optimizer
criterion = CombinedLoss(l1_weight=0.2, ssim_weight=0.5, perceptual_weight=0.3)
optimizer = optim.Adam(
    model.parameters(),
    lr=CONFIG['learning_rate'],
    weight_decay=1e-5
)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=5
)

print(f"‚úì Loss: L1 (0.2) + SSIM (0.5) + Perceptual (0.3)" if SSIM_AVAILABLE else "‚úì Loss: L1 only")
print(f"‚úì Optimizer: Adam (lr={CONFIG['learning_rate']})")
print(f"‚úì Scheduler: ReduceLROnPlateau")
print("\n‚úÖ Ready to train!")

# ============================================================
# VISUALIZATION HELPER
# ============================================================

def denormalize(img):
    """Denormalize image for display."""
    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(img.device)
    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(img.device)
    return (img * std + mean).clamp(0, 1)


def visualize_results(epoch):
    """Show model predictions vs ground truth."""
    model.eval()
    batch = next(iter(val_loader))

    with torch.no_grad():
        rgb = batch['rgb'][:2].to(device)
        gt = batch['ground_truth'][:2].to(device)
        pred, flow, _ = model(rgb)

    # Denormalize for display
    rgb_vis = denormalize(rgb).cpu()
    pred_vis = denormalize(pred).cpu()
    gt_vis = denormalize(gt).cpu()

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    for i in range(2):
        axes[i, 0].imshow(rgb_vis[i].permute(1, 2, 0).numpy())
        axes[i, 0].set_title('Input (Warped)', fontsize=12, fontweight='bold')
        axes[i, 0].axis('off')

        axes[i, 1].imshow(pred_vis[i].permute(1, 2, 0).numpy())
        axes[i, 1].set_title('Prediction', fontsize=12, fontweight='bold')
        axes[i, 1].axis('off')

        axes[i, 2].imshow(gt_vis[i].permute(1, 2, 0).numpy())
        axes[i, 2].set_title('Ground Truth', fontsize=12, fontweight='bold')
        axes[i, 2].axis('off')

    plt.suptitle(f'Epoch {epoch}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(checkpoint_dir / f'epoch_{epoch:03d}.png', dpi=100, bbox_inches='tight')
    plt.show()
    plt.close()

print("‚úì Visualization helper ready")

# ============================================================
# TRAINING LOOP WITH SSIM TRACKING & EARLY STOPPING
# ============================================================

import torch
from tqdm import tqdm

print("="*60)
print(f"üöÄ Starting Training - {CONFIG['num_epochs']} Epochs")
print("="*60)

train_losses = []
val_losses = []
train_ssims = []  # NEW: Track SSIM
val_ssims = []    # NEW: Track SSIM
best_val_loss = float('inf')
best_val_ssim = 0.0  # NEW
patience_counter = 0

EARLY_STOPPING_PATIENCE = 10

# Helper function to compute SSIM
def compute_ssim_batch(pred, target):
    """Compute average SSIM for a batch."""
    if SSIM_AVAILABLE:
        with torch.no_grad():
            return ssim(pred, target, data_range=1.0).item()
    return 0.0

for epoch in range(1, CONFIG['num_epochs'] + 1):
    print(f"\n{'='*60}")
    print(f"Epoch {epoch}/{CONFIG['num_epochs']}")
    print(f"{'='*60}")

    # ===================== TRAIN =====================
    model.train()
    epoch_loss = 0.0
    epoch_ssim = 0.0  # NEW
    train_pbar = tqdm(train_loader, desc="Training", leave=False)

    for batch_idx, batch in enumerate(train_pbar):
        rgb = batch['rgb'].to(device)
        gt = batch['ground_truth'].to(device)
        mask = batch['border'].to(device)

        optimizer.zero_grad()
        rectified, flow, _ = model(rgb)

        # Use perceptual loss if available
        if 'perceptual_loss' in globals():
            loss = criterion(rectified, gt, mask, perceptual_fn=perceptual_loss)
        else:
            loss = criterion(rectified, gt, mask)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        epoch_loss += loss.item()
        epoch_ssim += compute_ssim_batch(rectified, gt)  # NEW

        train_pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}',
            'ssim': f'{epoch_ssim/(batch_idx+1):.3f}'  # NEW
        })

    avg_train_loss = epoch_loss / len(train_loader)
    avg_train_ssim = epoch_ssim / len(train_loader)  # NEW
    train_losses.append(avg_train_loss)
    train_ssims.append(avg_train_ssim)  # NEW

    # =================== VALIDATION ===================
    model.eval()
    val_loss = 0.0
    val_ssim_score = 0.0  # NEW
    val_pbar = tqdm(val_loader, desc="Validation", leave=False)

    with torch.no_grad():
        for batch_idx, batch in enumerate(val_pbar):
            rgb = batch['rgb'].to(device)
            gt = batch['ground_truth'].to(device)
            mask = batch['border'].to(device)

            rectified, flow, _ = model(rgb)
            loss = criterion(rectified, gt, mask)
            val_loss += loss.item()
            val_ssim_score += compute_ssim_batch(rectified, gt)  # NEW

            val_pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'ssim': f'{val_ssim_score/(batch_idx+1):.3f}'  # NEW
            })

    avg_val_loss = val_loss / len(val_loader)
    avg_val_ssim = val_ssim_score / len(val_loader)  # NEW
    val_losses.append(avg_val_loss)
    val_ssims.append(avg_val_ssim)  # NEW

    # =================== LR SCHEDULER ===================
    scheduler.step(avg_val_loss)
    current_lr = optimizer.param_groups[0]['lr']

    # =================== CHECKPOINT ===================
    is_best = avg_val_loss < best_val_loss
    if is_best:
        best_val_loss = avg_val_loss
        best_val_ssim = avg_val_ssim  # NEW
        patience_counter = 0
    else:
        patience_counter += 1

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': avg_train_loss,
        'val_loss': avg_val_loss,
        'train_ssim': avg_train_ssim,  # NEW
        'val_ssim': avg_val_ssim,      # NEW
        'config': CONFIG,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_ssims': train_ssims,    # NEW
        'val_ssims': val_ssims          # NEW
    }

    # Save checkpoints
    torch.save(checkpoint, checkpoint_dir / 'latest_checkpoint.pth')
    if is_best:
        torch.save(checkpoint, checkpoint_dir / 'best_model.pth')
        print(f"  ‚ú® NEW BEST! (SSIM: {avg_val_ssim:.4f})")  # NEW
    if epoch % CONFIG['save_every'] == 0:
        torch.save(checkpoint, checkpoint_dir / f'checkpoint_epoch_{epoch:03d}.pth')
        print(f"  ‚úì Saved periodic checkpoint")

    # =================== VISUALIZATION ===================
    if epoch % 5 == 0 or epoch == 1:
        print(f"\nüì∏ Generating visualization...")
        visualize_results(epoch)

    # =================== EARLY STOPPING ===================
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"\n‚èπ Early stopping triggered after {epoch} epochs (no improvement for {EARLY_STOPPING_PATIENCE} epochs)")
        break

    # =================== LOG RESULTS WITH SSIM ===================
    print(f"\nüìä Epoch {epoch} Results:")
    print(f"  Train Loss: {avg_train_loss:.4f}  |  Train SSIM: {avg_train_ssim:.4f}")  # NEW
    print(f"  Val Loss:   {avg_val_loss:.4f}  |  Val SSIM:   {avg_val_ssim:.4f}")      # NEW
    print(f"  LR:         {current_lr:.6f}")

    # Quality assessment
    if avg_val_ssim > 0.85:
        print(f"  üéØ Quality: EXCELLENT")
    elif avg_val_ssim > 0.75:
        print(f"  ‚úÖ Quality: GOOD")
    elif avg_val_ssim > 0.65:
        print(f"  ‚ö†Ô∏è  Quality: FAIR")
    else:
        print(f"  ‚ùå Quality: NEEDS IMPROVEMENT")

    print("-"*60)

# =================== TRAINING COMPLETE ===================
print("\n" + "="*60)
print("üéâ Training Complete!")
print("="*60)
print(f"Best Validation Loss: {best_val_loss:.4f}")
print(f"Best Validation SSIM: {best_val_ssim:.4f}")  # NEW
print(f"Final LR: {optimizer.param_groups[0]['lr']:.6f}")

# ============================================================
# PLOT TRAINING CURVES WITH SSIM
# ============================================================

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Loss curves
axes[0].plot(train_losses, label='Train Loss', linewidth=2, marker='o', markersize=3)
axes[0].plot(val_losses, label='Val Loss', linewidth=2, marker='s', markersize=3)
axes[0].axhline(best_val_loss, color='r', linestyle='--', alpha=0.5,
                label=f'Best ({best_val_loss:.4f})')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# SSIM curves (NEW)
axes[1].plot(train_ssims, label='Train SSIM', linewidth=2, marker='o', markersize=3)
axes[1].plot(val_ssims, label='Val SSIM', linewidth=2, marker='s', markersize=3)
axes[1].axhline(0.85, color='g', linestyle='--', alpha=0.5, label='Excellent (0.85)')
axes[1].axhline(0.75, color='orange', linestyle='--', alpha=0.5, label='Good (0.75)')
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('SSIM', fontsize=12)
axes[1].set_title('Structural Similarity (Higher = Better)', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

# Validation improvement
improvements = [val_losses[0]]
for i in range(1, len(val_losses)):
    if val_losses[i] < min(val_losses[:i]):
        improvements.append(val_losses[i])
    else:
        improvements.append(improvements[-1])

axes[2].plot(val_losses, label='Val Loss', linewidth=2, alpha=0.6)
axes[2].plot(improvements, label='Best So Far', linewidth=2, linestyle='--')
axes[2].set_xlabel('Epoch', fontsize=12)
axes[2].set_ylabel('Validation Loss', fontsize=12)
axes[2].set_title('Validation Progress', fontsize=14, fontweight='bold')
axes[2].legend(fontsize=10)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(checkpoint_dir / 'training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"‚úì Curves saved to: {checkpoint_dir / 'training_curves.png'}")

# Final comparison
print("Generating final visualization...")
visualize_results(CONFIG['num_epochs'])

print("\n‚úÖ Training complete! Next steps:")
print("  1. Download 'best_model.pth' from checkpoints/")
print("  2. Run evaluation locally")
print("  3. Create your report")

from google.colab import files

# Download the best model
files.download('checkpoints/best_model.pth')
print("downloaded model")

"""______________________________________

"""

# #training
# import torch.nn as nn
# import torch.optim as optim
# from tqdm.notebook import tqdm
# from pathlib import Path

# try:
#     from pytorch_msssim import ssim
#     SSIM_AVAILABLE = True
# except:
#     SSIM_AVAILABLE = False
#     print("Warning: SSIM not available, using L1 only")

# # Create checkpoint directory
# Path('checkpoints').mkdir(exist_ok=True)

# # Load full dataset
# print("\nLoading dataset...")
# train_loader, val_loader = get_dataloaders(
#     data_dir=DATA_DIR,
#     batch_size=CONFIG['batch_size'],
#     img_size=(CONFIG['img_size'], CONFIG['img_size']),
#     use_border=True
# )

# # Create model
# model = DocumentUnwarpModel(flow_scale=1.0).to(device)

# # Loss function
# class CombinedLoss(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.l1 = nn.L1Loss(reduction='none')

#     def forward(self, pred, target, mask=None):
#         l1_loss = self.l1(pred, target)
#         if mask is not None:
#             # Expand mask to match l1_loss dimensions
#             mask = mask.expand_as(l1_loss)  # [B, 1, H, W] ‚Üí [B, 3, H, W]
#             # Compute masked average
#             l1_loss = (l1_loss * mask).sum() / (mask.sum() + 1e-8)
#         else:
#             # No mask - average all pixels
#             l1_loss = l1_loss.mean()


#         if SSIM_AVAILABLE:
#             ssim_loss = 1 - ssim(pred, target, data_range=1.0)
#             return 0.3 * l1_loss + 0.7 * ssim_loss
#         else:
#             return l1_loss

# criterion = CombinedLoss()
# optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])

# # Training
# print(f"\nStarting training for {CONFIG['num_epochs']} epochs...")
# train_losses = []
# val_losses = []
# best_val_loss = float('inf')

# for epoch in range(1, CONFIG['num_epochs'] + 1):
#     print(f"\n{'='*60}")
#     print(f"Epoch {epoch}/{CONFIG['num_epochs']}")
#     print(f"{'='*60}")

#     # TRAIN
#     model.train()
#     epoch_loss = 0.0

#     for batch in tqdm(train_loader, desc="Training"):
#         rgb = batch['rgb'].to(device)
#         gt = batch['ground_truth'].to(device)
#         mask = batch['border'].to(device)

#         optimizer.zero_grad()
#         rectified, flow, _ = model(rgb)
#         loss = criterion(rectified, gt, mask)
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#         optimizer.step()

#         epoch_loss += loss.item()

#     avg_train_loss = epoch_loss / len(train_loader)
#     train_losses.append(avg_train_loss)

#     # VALIDATE
#     model.eval()
#     val_loss = 0.0

#     with torch.no_grad():
#         for batch in tqdm(val_loader, desc="Validation"):
#             rgb = batch['rgb'].to(device)
#             gt = batch['ground_truth'].to(device)
#             mask = batch['border'].to(device)

#             rectified, flow, _ = model(rgb)
#             loss = criterion(rectified, gt, mask)
#             val_loss += loss.item()

#     avg_val_loss = val_loss / len(val_loader)
#     val_losses.append(avg_val_loss)

#     print(f"\nTrain Loss: {avg_train_loss:.4f}")
#     print(f"Val Loss:   {avg_val_loss:.4f}")

#     # Save best model
#     if avg_val_loss < best_val_loss:
#         best_val_loss = avg_val_loss
#         torch.save({
#             'epoch': epoch,
#             'model_state_dict': model.state_dict(),
#             'optimizer_state_dict': optimizer.state_dict(),
#             'val_loss': avg_val_loss,
#             'config': CONFIG
#         }, 'checkpoints/best_model.pth')
#         print(f"‚úì Saved best model (val_loss: {avg_val_loss:.4f})")

#     # Visualize results every 5 epochs
#     if epoch % 5 == 0:
#         model.eval()
#         batch = next(iter(val_loader))
#         with torch.no_grad():
#             rgb = batch['rgb'][:2].to(device)
#             gt = batch['ground_truth'][:2].to(device)
#             pred, flow, _ = model(rgb)

#         fig, axes = plt.subplots(2, 3, figsize=(12, 8))
#         for i in range(2):
#             axes[i, 0].imshow(rgb[i].cpu().permute(1, 2, 0).numpy())
#             axes[i, 0].set_title('Input')
#             axes[i, 0].axis('off')

#             axes[i, 1].imshow(pred[i].cpu().permute(1, 2, 0).clamp(0, 1).numpy())
#             axes[i, 1].set_title('Prediction')
#             axes[i, 1].axis('off')

#             axes[i, 2].imshow(gt[i].cpu().permute(1, 2, 0).numpy())
#             axes[i, 2].set_title('Ground Truth')
#             axes[i, 2].axis('off')

#         plt.tight_layout()
#         plt.savefig(f'checkpoints/epoch_{epoch:03d}.png', dpi=100, bbox_inches='tight')
#         plt.show()

# print("\n‚úì Training complete!")
# print(f"Best validation loss: {best_val_loss:.4f}")

# # plot
# plt.figure(figsize=(10, 5))
# plt.plot(range(1, len(train_losses)+1), train_losses, 'o-', label='Train Loss', linewidth=2)
# plt.plot(range(1, len(val_losses)+1), val_losses, 's-', label='Val Loss', linewidth=2)
# plt.xlabel('Epoch', fontsize=12)
# plt.ylabel('Loss', fontsize=12)
# plt.title('Training Progress', fontsize=14, fontweight='bold')
# plt.legend(fontsize=11)
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.savefig('checkpoints/training_curves.png', dpi=150, bbox_inches='tight')
# plt.show()

# from google.colab import files

# # Download the best model
# files.download('checkpoints/best_model.pth')
# print("downloaded model")



# # =========================================================================
# # STEP 1: INSTALLATION & IMPORTS
# # =========================================================================

# # Install the system utility for rendering graphs
# !apt install -y graphviz

# # Install the PyTorch visualization library
# !pip install torchview

# # Standard PyTorch and visualization imports
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import timm
# from torchview import draw_graph
# import numpy as np

# # --- ENVIRONMENT SETUP ---
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# print(f"Using device: {device}")

# # Define a configuration (matching typical usage in your training script)
# CONFIG = {
#     'img_size': 256, # Match your model's expected input resolution
# }


# # =========================================================================
# # STEP 2: DEFINE NECESSARY EXTERNAL FUNCTIONS
# # =========================================================================

# # The UnwarpUNet class calls 'create_base_grid', which is outside of model.py.
# # We must define it here as a placeholder for the visualization code to run.
# def create_base_grid(B, H, W, device):
#     """Generates a regular meshgrid in the normalized [-1, 1] coordinate space."""
#     grid_y, grid_x = torch.meshgrid(torch.linspace(-1, 1, H, device=device),
#                                     torch.linspace(-1, 1, W, device=device),
#                                     indexing='ij')
#     # Stack and add batch dimension: [H, W, 2] -> [B, H, W, 2]
#     grid = torch.stack((grid_x, grid_y), dim=-1).unsqueeze(0).repeat(B, 1, 1, 1)
#     return grid

# # =========================================================================
# # STEP 3: THE UnwarpUNet CLASS (FROM YOUR model.py)
# # =========================================================================

# class DocumentUnwarpModel(nn.Module):
#     """
#     U-Net architecture with ResNet50 encoder for predicting flow fields for unwarping.
#     """

#     def __init__(self, flow_scale=1.0):
#         super().__init__()
#         self.flow_scale = flow_scale

#         # 1. Pretrained Encoder (ResNet50)
#         self.encoder = timm.create_model(
#             "resnet50",
#             pretrained=True,
#             features_only=True,
#             out_indices=(1, 2, 3, 4)
#         )
#         encoder_channels = [256, 512, 1024, 2048]

#         # 2. Decoder (U-Net style with skip connections)
#         self.up4 = self._up_block(encoder_channels[3], encoder_channels[2])
#         self.up3 = self._up_block(encoder_channels[2], encoder_channels[1])
#         self.up2 = self._up_block(encoder_channels[1], encoder_channels[0])
#         self.up1 = self._up_block(encoder_channels[0], 128)

#         # Final upsampling to match input resolution
#         self.final_up = nn.Sequential(
#             nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True),
#             nn.Conv2d(128, 64, 3, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(64, 64, 3, padding=1),
#             nn.ReLU(inplace=True)
#         )

#         # 3. Flow Prediction Head
#         self.flow_head = nn.Sequential(
#             nn.Conv2d(64, 32, 3, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(32, 2, 3, padding=1)
#         )

#         nn.init.zeros_(self.flow_head[-1].weight)
#         nn.init.zeros_(self.flow_head[-1].bias)

#     def _up_block(self, in_ch, out_ch):
#         """Upsampling block for decoder."""
#         return nn.Sequential(
#             nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True),
#             nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
#             nn.BatchNorm2d(out_ch),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
#             nn.BatchNorm2d(out_ch),
#             nn.ReLU(inplace=True)
#         )

#     def _match_shapes(self, x, target):
#         """Resize x to match target's spatial dimensions."""
#         if x.shape[2:] != target.shape[2:]:
#             x = F.interpolate(
#                 x,
#                 size=target.shape[2:],
#                 mode='bilinear',
#                 align_corners=True
#             )
#         return x

#     def forward(self, x):
#         """Forward pass: predict flow field and warp input image."""
#         B, C, H, W = x.shape

#         # STEP 1: Encoder - Extract hierarchical features
#         feats = self.encoder(x)
#         f1, f2, f3, f4 = feats

#         # STEP 2: Decoder - Upsample with skip connections
#         d4 = self.up4(f4)
#         d4 = self._match_shapes(d4, f3)
#         d4 = d4 + f3

#         d3 = self.up3(d4)
#         d3 = self._match_shapes(d3, f2)
#         d3 = d3 + f2

#         d2 = self.up2(d3)
#         d2 = self._match_shapes(d2, f1)
#         d2 = d2 + f1

#         d1 = self.up1(d2)
#         d0 = self.final_up(d1)
#         d0 = self._match_shapes(d0, x)

#         # STEP 3: Predict Flow Field
#         flow = self.flow_head(d0)
#         flow = torch.tanh(flow) * self.flow_scale

#         # STEP 4: Create Sampling Grid
#         base_grid = create_base_grid(B, H, W, x.device)
#         sampling_grid = base_grid + flow.permute(0, 2, 3, 1)

#         # STEP 5: Warp Input Image
#         rectified = F.grid_sample(
#             x,
#             sampling_grid,
#             mode="bilinear",
#             padding_mode="border",
#             align_corners=True
#         )

#         return rectified, flow, sampling_grid


# # =========================================================================
# # STEP 4: GENERATE AND DISPLAY GRAPH
# # =========================================================================

# # 1. Initialize the model
# model = DocumentUnwarpModel().to(device)
# model.eval()

# # 2. Create a sample input tensor: [Batch Size, Channels, Height, Width]
# BATCH_SIZE = 1
# input_data = torch.randn(BATCH_SIZE, 3, CONFIG['img_size'], CONFIG['img_size']).to(device)

# # 3. Generate the graph
# print("\nGenerating model graph (UnwarpUNet_Architecture.png)...")
# model_graph = draw_graph(
#     model,
#     input_data=input_data,
#     graph_name='UnwarpUNet_Architecture',
#     save_graph=True,
#     # Use a depth of 4 to clearly see the blocks (encoder, up_block)
#     depth=4,
#     expand_nested=True
# )

# # 4. Display the graph image (will also be saved as 'UnwarpUNet_Architecture.png')
# model_graph.visual_graph